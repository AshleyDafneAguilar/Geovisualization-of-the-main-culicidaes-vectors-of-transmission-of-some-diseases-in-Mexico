# -*- coding: utf-8 -*-
"""Proyecto_Ecologia.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1hCgUbryUmD2SsMGET3t-f2G8piYvREYN

#Librerías
"""

import pandas as pd
import numpy as np

"""#Datos

## Cargar Datos

Se cargan los datos de las especies con las que se trabjara:

- lutzomyia
  - olmeca
  - cruciata
- aegypti
- Anopheles
  - albimanus
  - pseudopunctipennis
"""

from google.colab import drive
drive.mount('/content/drive')

"""### Lutzomyia"""

lutzomyia = pd.read_csv('/content/drive/MyDrive/Ecologia/Lutzomyia.csv'); lutzomyia

"""---

### Aedes
"""

aedes = pd.read_csv('/content/drive/MyDrive/Ecologia/Aedes.csv'); aedes

aegypti = pd.read_csv('/content/drive/MyDrive/Ecologia/Aedes_aegypti.csv'); aegypti

"""---

### Anopheles
"""

albimanus =  pd.read_csv('/content/drive/MyDrive/Ecologia/Anopheles_albimanus.csv'); albimanus

pseudopunctipennis = pd.read_csv('/content/drive/MyDrive/Ecologia/Anopheles_pseudopunctipennis.csv'); pseudopunctipennis

"""## Limpieza de datos

- Nombre especie
- País México MX
- Insertidumbre <100
- Coordenadas
- Año/ eventday

**En segmento por año elimina los puntos iguales**

### Lutzomyia

#### **Columnas**

Nos quedamos con las columnas que se consideran importantes: *family, genus,verbatimScientificName, countryCode, locality, stateProvince, decimalLatitude,decimalLongitude, coordinateUncertaintyInMeters, elevation, year*
"""

lutzomyia.columns.values

lutzomyia = lutzomyia.loc[:, ['verbatimScientificName', 'countryCode',
                              'decimalLatitude','decimalLongitude', 'coordinateUncertaintyInMeters', 'year']]

"""---

#### **División de datos**

De acuerdo al artículo [4], el agente causal de la Leishmania mexicana es transmitido a humanos por la picadura del insecto vector Lutzomyia olmeca olmeca y  potencialmente también por  Lutzomyia cruciata, por lo cual el DatFrame se reducira a estas dos especies en base a la columna: *verbatimScientificName*
"""

lutzomyia.verbatimScientificName.unique()

olmeca =  lutzomyia[lutzomyia.verbatimScientificName == "Lutzomyia olmeca subsp. olmeca"]; olmeca

cruciata = lutzomyia[lutzomyia.verbatimScientificName == "Lutzomyia (Tricholateralis) cruciata"]; cruciata

"""---

#### **Olmeca**

- Datos nulos
"""

olmeca.isnull().sum()

"""---

- Segmentación por año

>[1970-1985)
>
>[1985-2000)
>
>[2000-2012)
>
>[2015-2024]
"""

min(olmeca.year), max(olmeca.year)

#no hay datos entre esa fecha
#olmeca_1970_1985 = olmeca[(olmeca.year >= 1970) & (olmeca.year < 1985)]

olmeca_1985_2000 = olmeca[(olmeca.year >= 1985) & (olmeca.year < 2000)]
olmeca_2000_2015 = olmeca[(olmeca.year >= 2000) & (olmeca.year < 2015)]

#no hay datos entre esa fecha
#olmeca_2015_2024 = olmeca[(olmeca.year >= 2015) & (olmeca.year < 2024)]

"""---

- Datos duplicados
"""

#hay datos nulos
olmeca_1985_2000.duplicated().unique()

olmeca_1985_2000 = olmeca_1985_2000.drop_duplicates()

#hay datos nulos
olmeca_2000_2015.duplicated().unique()

olmeca_2000_2015 = olmeca_2000_2015.drop_duplicates()

"""---

- Guardar en .csv
"""

olmeca.to_csv('olmeca.csv', index=False)
olmeca_1985_2000.to_csv('olmeca(1985-2000).csv', index=False)
olmeca_2000_2015.to_csv('olmeca(2000-2015).csv', index=False)

"""----

---

#### **Cruciata**

- Datos nulos
"""

cruciata.isnull().sum()

"""---

- Segmentación por año

>[1970-1985)
>
>[1985-2000)
>
>[2000-2012)
>
>[2015-2024]
"""

min(cruciata.year), max(cruciata.year)

#no hay datos entre esa fecha
#cruciata_1970_1985 = cruciata[(cruciata).year >= 1970) & (cruciata.year < 1985)]

cruciata_1985_2000 = cruciata[(cruciata.year >= 1985) & (cruciata.year < 2000)]
cruciata_2000_2015 = cruciata[(cruciata.year >= 2000) & (cruciata.year < 2015)]

#no hay datos entre esa fecha
#cruciata_2015_2024 = cruciata[(cruciata.year >= 2015) & (cruciata.year < 2024)]

"""---

- Datos duplicados
"""

#hay datos duplicados
cruciata_1985_2000.duplicated().unique()

cruciata_1985_2000 = cruciata_1985_2000.drop_duplicates()

# hay datos duplicados
cruciata_2000_2015.duplicated().unique()

cruciata_2000_2015 = cruciata_2000_2015.drop_duplicates()

"""---

- Guardar en .csv
"""

cruciata.to_csv('cruciata.csv', index=False)
cruciata_1985_2000.to_csv('cruciata(1985-2000).csv', index=False)
cruciata_2000_2015.to_csv('cruciata(2000-2015).csv', index=False)

"""---

### Aedes

#### **Aedes especies**
>- Aedes brelandi Zavortink
- Aedes hendersoni
- Aedes triseriatus
- Aedes zoosophus

- Columnas
"""

aedes.columns.values

aedes = aedes.loc[:, ['verbatimScientificName','decimalLatitude','decimalLongitude',
                              'coordinateUncertaintyInMeters', 'eventDate', 'year']]

"""---

- División de datos

De acuerdo al artículo [12] **Aedes brelandi Zavortink** se reporta por primera vez fuera de los Estados Unidos, donde se ha encontrado en el norte y zonas centrales de México así como **Ae. hendersoni**. **Ae. zoosophus Dyar y Knab** está registrado en sur de México. Se incluye el grupo **Aedes Triseriatus**. Vectores causantes de enfermedades como leishmaniasis, malaria, dengue, etc.
"""

zoosophus  = aedes[aedes.verbatimScientificName.str.contains('zoosophus', case=False)]
brelandi = aedes[aedes.verbatimScientificName.str.contains('brelandi', case=False)]
hendersoni = aedes[aedes.verbatimScientificName.str.contains('hendersoni', case=False)]
triseriatus = aedes[aedes.verbatimScientificName.str.contains('triseriatus', case=False)]

"""---

##### Zoosophus

- Datos nulos
"""

zoosophus.isnull().sum()

"""Modificamos los valores nulos de la columna **coordinateUncertaintyInMeters** pensando en que fueron exactos al tomar las coordenadas."""

zoosophus.coordinateUncertaintyInMeters = zoosophus.coordinateUncertaintyInMeters.fillna(0)

"""Eliminamos la columna **eventDate** ya que tenemos todos los datos de la columna **year**"""

zoosophus = zoosophus.drop ('eventDate', axis = 1)

"""---

- Segmentación por año

>[1970-1985)
>
>[1985-2000)
>
>[2000-2012)
>
>[2015-2024]
"""

#primero cambiamos el tipo de dato de float a entero
zoosophus['year'] = zoosophus['year'].astype(int)

min(zoosophus.year), max(zoosophus.year)

zoosophus_1961_1970 = zoosophus[(zoosophus.year >= 1961) & (zoosophus.year < 1970)]
zoosophus_2000_2015 = zoosophus[(zoosophus.year >= 2000) & (zoosophus.year < 2015)]

#no hay datos entre esa fecha
#zoosophus_1970_1985 = zoosophus[(zoosophus.year >= 1970) & (zoosophus.year < 1985)]
#zoosophus_2015_2024 = zoosophus[(zoosophus.year >= 2015) & (zoosophus.year < 2024)]
#zoosophus_1985_2000 = zoosophus[(zoosophus.year >= 1985) & (zoosophus.year < 2000)]

"""---

- Datos duplicados
"""

# hay datos duplicados
zoosophus_1961_1970.duplicated().unique()

zoosophus_1961_1970 = zoosophus_1961_1970.drop_duplicates()

#no hay datos duplicados
zoosophus_2000_2015.duplicated().unique()

"""---

- Guardar en .csv
"""

zoosophus.to_csv('zoosophus.csv', index=False)
zoosophus_1961_1970.to_csv('zoosophus(1961-1970).csv', index=False)
zoosophus_2000_2015.to_csv('zoosophus(2000-2015).csv', index=False)

"""---

##### Brelandi

- Datos nulos
"""

brelandi.isnull().sum()

"""Modificamos los valores nulos de la columna **coordinateUncertaintyInMeters** pensando en que fueron exactos al tomar las coordenadas."""

brelandi.coordinateUncertaintyInMeters = brelandi.coordinateUncertaintyInMeters.fillna(0)

"""Eliminamos la columna **eventDate** ya que tenemos todos los datos de la columna **year**"""

brelandi = brelandi.drop ('eventDate', axis = 1)

"""---

- Segmentación por año

>[1970-1985)
>
>[1985-2000)
>
>[2000-2012)
>
>[2015-2024]
"""

min(brelandi.year), max(brelandi.year)

brelandi_2000_2015 = brelandi[(brelandi.year >= 2000) & (brelandi.year < 2015)]

#no hay datos entre esa fecha
#brelandi_1970_1985 = brelandi[(brelandi.year >= 1970) & (brelandi.year < 1985)]
#brelandi_1985_2000 = brelandi[(brelandi.year >= 2015) & (brelandi.year < 2024)]
#brelandi_2015_2024 = brelandi[(brelandi.year >= 1985) & (brelandi.year < 2000)]

"""---

- Datos duplicados
"""

#no hay datos duplicados
brelandi_2000_2015.duplicated().unique()

"""---

- Guardar en .csv
"""

brelandi.to_csv('brelandi.csv', index=False)
brelandi_2000_2015.to_csv('brelandi(2000-2015).csv', index=False)

"""---

##### Hendersoni

- Datos nulos
"""

hendersoni.isnull().sum()

"""Eliminamos la columna **eventDate** ya que tenemos todos los datos de la columna **year**"""

hendersoni = hendersoni.drop ('eventDate', axis = 1)

"""---

- Segmentación por año

>[1970-1985)
>
>[1985-2000)
>
>[2000-2012)
>
>[2015-2024]
"""

#primero cambiamos el tipo de dato de float a entero
hendersoni['year'] = hendersoni['year'].astype(int)

min(hendersoni.year), max(hendersoni.year)

hendersoni_1970_1985 = hendersoni[(hendersoni.year >= 1970) & (hendersoni.year < 1985)]

#no hay datos entre esa fecha
#hendersoni_1985_2000 = hendersoni[(hendersoni.year >= 1985) & (hendersoni.year < 200)]
#hendersoni_2000_2015 = hendersoni[(hendersoni.year >= 2000) & (hendersoni.year < 2015)]
#hendersoni_2015_2024 = hendersoni[(hendersoni.year >= 2015) & (hendersoni.year < 2024)]

"""---

- Datos duplicados
"""

# hay datos duplicados
hendersoni_1970_1985.duplicated().unique()

"""---

- Guardar en .csv
"""

hendersoni.to_csv('hendersoni.csv', index=False)
hendersoni_1970_1985.to_csv('hendersoni(1970-1985).csv', index=False)

"""---

##### Triseriatus

- Datos nulos
"""

triseriatus.isnull().sum()

"""Modificamos los valores nulos de la columna **coordinateUncertaintyInMeters** pensando en que fueron exactos al tomar las coordenadas."""

triseriatus.coordinateUncertaintyInMeters = triseriatus.coordinateUncertaintyInMeters.fillna(0)

"""Eliminamos el único dato nulo, ya que no hay manera de obtener el año"""

triseriatus = triseriatus.dropna()

"""Eliminamos la columna **eventDate** ya que tenemos todos los datos de la columna **year**"""

triseriatus = triseriatus.drop ('eventDate', axis = 1)

"""---

- Segmentación por año

>[1970-1985)
>
>[1985-2000)
>
>[2000-2012)
>
>[2015-2024]
"""

#primero cambiamos el tipo de dato de float a entero
triseriatus['year'] = triseriatus['year'].astype(int)

min(triseriatus.year), max(triseriatus.year)

triseriatus_1954_1970 = triseriatus[(triseriatus.year >= 1954) & (triseriatus.year < 1970)]
triseriatus_1985_2000 = triseriatus[(triseriatus.year >= 1985) & (triseriatus.year < 2000)]
triseriatus_2000_2015 = triseriatus[(triseriatus.year >= 200) & (triseriatus.year < 20015)]

#no hay datos entre esa fecha
#triseriatus_1970_1985 = triseriatus[(triseriatus.year >= 1970) & (triseriatus.year < 1985)]
#triseriatus_2015_2024 = triseriatus[(triseriatus.year >= 2015) & (triseriatus.year < 2024]

"""---

- Datos duplicados
"""

# hay datos duplicados
triseriatus_1954_1970.duplicated().unique()

triseriatus_1954_1970 = triseriatus_1954_1970.drop_duplicates()

# hay datos duplicados
triseriatus_1985_2000.duplicated().unique()

triseriatus_1985_2000 = triseriatus_1985_2000.drop_duplicates()

# hay datos duplicados
triseriatus_2000_2015.duplicated().unique()

triseriatus_2000_20015 = triseriatus_2000_2015.drop_duplicates()

"""---

- Guardar en .csv
"""

triseriatus.to_csv('triseriatus.csv', index=False)
triseriatus_1954_1970.to_csv('triseriatus(1954-1970).csv', index=False)
triseriatus_1985_2000.to_csv('triseriatus(1985-2000).csv', index=False)
triseriatus_2000_2015.to_csv('triseriatus(2000-2015).csv', index=False)

"""---

#### **Aegypti**

- Columnas
"""

aegypti.columns.values

"""Vemos que tenemos los mismos valores nulos de la columna **year** y la columna **eventDate** por lo cual no podemos obtener el año de acuerdo a la fecha y por eso no se agrega esa columna"""

null_year = list(aegypti[aegypti.year.isnull() == True].index)
null_eventDate = list(aegypti[aegypti.eventDate.isnull() == True].index)

null_year == null_eventDate

aegypti = aegypti.loc[:, ['verbatimScientificName', 'countryCode','decimalLatitude','decimalLongitude',
                          'coordinateUncertaintyInMeters', 'year']]

"""---

- Datos de México
"""

aegypti.countryCode.unique()

aegypti = aegypti[aegypti.countryCode == 'MX']; aegypti

"""---

- Datos faltantes

Modificamos los valores nulos de la columna **coordinateUncertaintyInMeters** pensando en que fueron exactos al tomar las coordenadas.
"""

aegypti.coordinateUncertaintyInMeters = aegypti.coordinateUncertaintyInMeters.fillna(0)

aegypti

"""Ahora eliminamos las los registros que tienen valores nulos, en este caso y después del paso anterior, los únicos que se eliminan son los 51 datos que no contiene el año"""

aegypti.isnull().sum()

aegypti = aegypti.dropna()

aegypti

"""---

- Segmentación por año

>[1970-1985)
>
>[1985-2000)
>
>[2000-2012)
>
>[2015-2024]
"""

#primero cambiamos el tipo de dato de float a entero
aegypti['year'] = aegypti['year'].astype(int)

min(aegypti.year), max(aegypti.year)

aegypti_1970_1985 = aegypti[(aegypti.year >= 1970) & (aegypti.year < 1985)]
aegypti_1985_2000 = aegypti[(aegypti.year >= 1985) & (aegypti.year < 2000)]
aegypti_2000_2015 = aegypti[(aegypti.year >= 2000) & (aegypti.year < 2015)]

#no hay datos entre esa fecha
#aegypti_2015_2024 = aegypti[(aegypti.year >= 2015) & (aegypti.year < 2024)]

"""---

- Datos duplicados
"""

#no hay datos duplicados
aegypti_1970_1985.duplicated().unique()

#hay datos dubplicados
aegypti_1985_2000.duplicated().unique()

aegypti_1985_2000 = aegypti_1985_2000.drop_duplicates()

#hay datos duplicados
aegypti_2000_2015.duplicated().unique()

aegypti_2000_2015 = aegypti_2000_2015.drop_duplicates()

"""---

- Guardar en .csv
"""

aegypti.to_csv('aegypti.csv', index=False)
aegypti_1970_1985.to_csv('aegypti(1970-1985).csv', index=False)
aegypti_1985_2000.to_csv('aegypti(1985-2000).csv', index=False)
aegypti_2000_2015.to_csv('aegypti(2000-2015).csv', index=False)

"""---

### Anopheles

#### **Albimanus**

- Columnas
"""

albimanus.columns.values

albimanus = albimanus.loc[:, ['verbatimScientificName','decimalLatitude','decimalLongitude',
                              'coordinateUncertaintyInMeters', 'eventDate', 'year']]

"""---

- Selección de especie

Nos quedamos con aquellos datos donde en la columna **verbatimScientificName** contengan la palabra *albimanus*
"""

albimanus.verbatimScientificName.unique()

#case = False:  ignore la diferencias entre mayúsculas y minúsculas.
albimanus = albimanus[albimanus.verbatimScientificName.str.contains('albimanus', case=False)]

"""---

- Datos Nulos
"""

albimanus.isnull().sum()

"""Modificamos los valores nulos de la columna **coordinateUncertaintyInMeters** pensando en que fueron exactos al tomar las coordenadas."""

albimanus.coordinateUncertaintyInMeters = albimanus.coordinateUncertaintyInMeters.fillna(0)

albimanus.coordinateUncertaintyInMeters.unique()

"""Observamos que hay datos mayores a 100 en la incertidumbre, estos datos serán eliminados."""

albimanus = albimanus[albimanus.coordinateUncertaintyInMeters <= 100]

albimanus

"""---

- Modificar datos

En la sección anterior observamos que hay más datos nulos en la columna **year** que en la columna **eventDate**. Como el dato de año es importante para este trabajo se decide obtener este elemento de la columna **eventDate** con la finalidad de no perder demasiados datos.
"""

#observar formato
albimanus.eventDate.unique()

#borramos los datos nulos de eventDate
albimanus = albimanus.dropna(subset = ['eventDate']); albimanus

#selecciona unicamente el año
#de la columna eventDate

date = albimanus.eventDate
year = pd.Series([int(i[:4]) for i in date]); year

#al tener todos los valores del año
#borramos las columnas eventDate y year

albimanus = albimanus.drop ('year', axis = 1)
albimanus = albimanus.drop ('eventDate', axis = 1)

#agregamos los valores de year
albimanus['year'] = list(year)

#ya no hay valores nulos
albimanus.isnull().sum()

"""---

- Segmentación por año

>[1970-1985)
>
>[1985-2000)
>
>[2000-2012)
>
>[2015-2024]
"""

min(albimanus.year), max(albimanus.year)

albimanus_1902_1970 = albimanus[(albimanus.year >= 1902) & (albimanus.year < 1970)] #Se  guardan los datos que fueron  registrados antes de 1970
albimanus_1970_1985 = albimanus[(albimanus.year >= 1970) & (albimanus.year < 1985)]
albimanus_1985_2000 = albimanus[(albimanus.year >= 1985) & (albimanus.year < 2000)]
albimanus_2000_2015 = albimanus[(albimanus.year >= 2000) & (albimanus.year < 2015)]
albimanus_2015_2024 = albimanus[(albimanus.year >= 2015) & (albimanus.year <= 2024)]

"""---

- Datos duplicados
"""

#hay datos duplicados
albimanus_1902_1970.duplicated().unique()

albimanus_1902_1970 = albimanus_1902_1970.drop_duplicates()

#hay datos duplicados
albimanus_1970_1985.duplicated().unique()

albimanus_1970_1985 = albimanus_1970_1985.drop_duplicates()

#hay datos duplicados
albimanus_1985_2000.duplicated().unique()

albimanus_1985_2000 = albimanus_1985_2000.drop_duplicates()

#hay datos duplicados
albimanus_2000_2015.duplicated().unique()

albimanus_2000_2015 = albimanus_2000_2015.drop_duplicates()

#hay datos duplicados
albimanus_2015_2024.duplicated().unique()

albimanus_2015_2024 = albimanus_2015_2024.drop_duplicates()

"""---

- Guardar en .csv
"""

albimanus.to_csv('albimanus.csv', index=False)
albimanus_1902_1970.to_csv('albimanus(1902-1970).csv', index=False)
albimanus_1970_1985.to_csv('albimanus(1970-1985).csv', index=False)
albimanus_1985_2000.to_csv('albimanus(1985-2000).csv', index=False)
albimanus_2000_2015.to_csv('albimanus(2000-2015).csv', index=False)
albimanus_2015_2024.to_csv('albimanus(2015-2024).csv', index=False)

"""---

#### **Pseudopunctipennis**

- Columnas
"""

pseudopunctipennis.columns.values

pseudopunctipennis = pseudopunctipennis.loc[:, ['verbatimScientificName','decimalLatitude','decimalLongitude',
                              'coordinateUncertaintyInMeters', 'eventDate', 'year']]

"""---

- Selección de especie

Nos quedamos con aquellos datos donde en la columna **verbatimScientificName** contengan la palabra *pseudopunctipennis*
"""

pseudopunctipennis.verbatimScientificName.unique()

#case = False:  ignore la diferencias entre mayúsculas y minúsculas.
pseudopunctipennis = pseudopunctipennis[pseudopunctipennis.verbatimScientificName.str.contains('pseudopunctipennis', case=False)]

"""---

- Datos Nulos
"""

pseudopunctipennis.isnull().sum()

"""Modificamos los valores nulos de la columna **coordinateUncertaintyInMeters** pensando en que fueron exactos al tomar las coordenadas."""

pseudopunctipennis.coordinateUncertaintyInMeters = pseudopunctipennis.coordinateUncertaintyInMeters.fillna(0)

pseudopunctipennis.coordinateUncertaintyInMeters.unique()

"""Observamos que hay datos mayores a 100 en la incertidumbre, estos datos serán eliminados."""

pseudopunctipennis = pseudopunctipennis[pseudopunctipennis.coordinateUncertaintyInMeters <= 100]; pseudopunctipennis

"""---

- Modificar datos

En la sección anterior observamos que hay más datos nulos en la columna **year** que en la columna **eventDate**. Como el dato de año es importante para este trabajo se decide obtener este elemento de la columna **eventDate** con la finalidad de no perder demasiados datos.
"""

#observar formato
pseudopunctipennis.eventDate.unique()

#borramos los datos nulos de eventDate
pseudopunctipennis = pseudopunctipennis.dropna(subset = ['eventDate']); pseudopunctipennis

#selecciona unicamente el año
#de la columna eventDate

date = pseudopunctipennis.eventDate
year = pd.Series([int(i[:4]) for i in date]); year

#al tener todos los valores del año
#borramos las columnas eventDate y year

pseudopunctipennis = pseudopunctipennis.drop ('year', axis = 1)
pseudopunctipennis = pseudopunctipennis.drop ('eventDate', axis = 1)

#agregamos los valores de year
pseudopunctipennis['year'] = list(year)

pseudopunctipennis.isnull().sum()

"""---

- Segmentación por año

>[1970-1985)
>
>[1985-2000)
>
>[2000-2012)
>
>[2015-2024]
"""

min(pseudopunctipennis.year), max(pseudopunctipennis.year)

pseudopunctipennis_1925_1970 = pseudopunctipennis[(pseudopunctipennis.year >= 1925) & (pseudopunctipennis.year < 1970)] #Se  guardan los datos que fueron  registrados antes de 1970
pseudopunctipennis_1970_1985 = pseudopunctipennis[(pseudopunctipennis.year >= 1970) & (pseudopunctipennis.year < 1985)]
pseudopunctipennis_1985_2000 = pseudopunctipennis[(pseudopunctipennis.year >= 1985) & (pseudopunctipennis.year < 2000)]
pseudopunctipennis_2000_2015 = pseudopunctipennis[(pseudopunctipennis.year >= 2000) & (pseudopunctipennis.year < 2015)]
pseudopunctipennis_2015_2024 = pseudopunctipennis[(pseudopunctipennis.year >= 2015) & (pseudopunctipennis.year <= 2024)]

"""---

- Datos duplicados
"""

#hay datos duplicados
pseudopunctipennis_1925_1970.duplicated().unique()

pseudopunctipennis_1925_1970 = pseudopunctipennis_1925_1970.drop_duplicates()

#hay datos duplicados
pseudopunctipennis_1970_1985.duplicated().unique()

pseudopunctipennis_1970_1985 = pseudopunctipennis_1970_1985.drop_duplicates()

#hay datos nulos
pseudopunctipennis_1985_2000.duplicated().unique()

pseudopunctipennis_1985_2000 = pseudopunctipennis_1985_2000.drop_duplicates()

#hay datos nulos
pseudopunctipennis_2000_2015.duplicated().unique()

pseudopunctipennis_2000_2015 = pseudopunctipennis_2000_2015.drop_duplicates()

#hay datos duplicados
pseudopunctipennis_2015_2024.duplicated().unique()

pseudopunctipennis_2015_2024 = pseudopunctipennis_2015_2024.drop_duplicates()

"""---

- Guardar en .csv
"""

pseudopunctipennis.to_csv('pseudopunctipennis.csv', index=False)
pseudopunctipennis_1925_1970.to_csv('pseudopunctipennis(1925-1970).csv', index=False)
pseudopunctipennis_1970_1985.to_csv('pseudopunctipennis(1970-1985).csv', index=False)
pseudopunctipennis_1985_2000.to_csv('pseudopunctipennis(1985-2000).csv', index=False)
pseudopunctipennis_2000_2015.to_csv('pseudopunctipennis(2000-2015).csv', index=False)
pseudopunctipennis_2015_2024.to_csv('pseudopunctipennis(2015-2024).csv', index=False)
"""---

## Capas bioclimáticas

Concatenar la información del paso anterior, Limpieza de datos, con la información de las capas bioclimaticas obtenidas en QGIS

### Aegypti

#### Concatenación de datos
"""

aegypti_capas = pd.read_csv('/content/drive/MyDrive/Ecologia/Dataset/dataset_capas/Aegypti.csv')

"""Vemos que coincide el orden de la información del Datframe que contiene los valores de las capas bioclimáticas de cada individuo con la información general antes seleccionada del individuo. Por lo que concatenamos estos dos Dataframes"""

#comprobación del orde
aegypti1 = aegypti_capas[['X', 'Y']]
aegypti2 = aegypti[['decimalLongitude', 'decimalLatitude']]

aegypti1.reset_index(drop=True, inplace=True)
aegypti2.reset_index(drop=True, inplace=True)

pd.concat([ aegypti1, aegypti2], axis=1)

#borro la columna X, Y de aegypti_capas
aegypti_capas.drop('X', axis = 1, inplace =True)
aegypti_capas.drop('Y', axis = 1, inplace = True)

aegypti_capas.reset_index(drop=True, inplace=True)

aegypti.reset_index(drop=True, inplace=True)

aegypti_capas = pd.concat([ aegypti, aegypti_capas], axis=1); aegypti_capas

"""---

#### Datos nulos
"""

aegypti_capas.isnull().sum()

#borramos los datos faltantes
aegypti_capas = aegypti_capas.dropna()
#borramos la columna fid
aegypti_capas = aegypti_capas.drop('fid', axis =1)

"""---

#### Segementación por año
>[1970-1985)
>
>[1985-2000)
>
>[2000-2012)
>
>[2015-2024]
"""

aegypti_capas_1970_1985 = aegypti_capas[(aegypti_capas.year >= 1970) & (aegypti_capas.year < 1985)]
aegypti_capas_1985_2000 = aegypti_capas[(aegypti_capas.year >= 1985) & (aegypti_capas.year < 2000)]
aegypti_capas_2000_2015 = aegypti_capas[(aegypti_capas.year >= 2000) & (aegypti_capas.year < 2015)]

"""---

#### Guarda .csv
"""

aegypti_capas.to_csv('aegypti_capas.csv', index=False)
aegypti_capas_1970_1985.to_csv('aegypti_capas(1970-1985).csv', index=False)
aegypti_capas_1985_2000.to_csv('aegypti_capas(1985-2000).csv', index=False)
aegypti_capas_2000_2015.to_csv('aegypti_capas(2000-2015).csv', index=False)

"""---

### Albimanus

#### Concatenación de datos
"""

albimanus_capas = pd.read_csv('/content/drive/MyDrive/Ecologia/Dataset/dataset_capas/Albimanus.csv'); albimanus_capas

"""Vemos que coincide el orden de la información del Datframe que contiene los valores de las capas bioclimáticas de cada individuo con la información general antes seleccionada del individuo. Por lo que concatenamos estos dos Dataframes"""

#comprobación del orde
albimanus1 = albimanus_capas[['X', 'Y']]
albimanus2 = albimanus[['decimalLongitude', 'decimalLatitude']]

albimanus1.reset_index(drop=True, inplace=True)
albimanus2.reset_index(drop=True, inplace=True)

pd.concat([ albimanus1, albimanus2], axis=1)

#borro la columna X, Y de albimanus_capas
albimanus_capas.drop('X', axis = 1, inplace =True)
albimanus_capas.drop('Y', axis = 1, inplace = True)

albimanus_capas.reset_index(drop=True, inplace=True)

albimanus.reset_index(drop=True, inplace=True)

albimanus_capas = pd.concat([ albimanus, albimanus_capas], axis=1); albimanus_capas

"""---

#### Datos nulos
"""

albimanus_capas.isnull().sum()

#borramos los datos faltantes
albimanus_capas = albimanus_capas.dropna()
#borramos la columna fid
albimanus_capas = albimanus_capas.drop('fid', axis =1)

"""---

#### Segementación por año
>[1970-1985)
>
>[1985-2000)
>
>[2000-2012)
>
>[2015-2024]
"""

albimanus_capas_1902_1970 = albimanus_capas[(albimanus_capas.year >= 1902) & (albimanus_capas.year < 1970)] #Se  guardan los datos que fueron  registrados antes de 1970
albimanus_capas_1970_1985 = albimanus_capas[(albimanus_capas.year >= 1970) & (albimanus_capas.year < 1985)]
albimanus_capas_1985_2000 = albimanus_capas[(albimanus_capas.year >= 1985) & (albimanus_capas.year < 2000)]
albimanus_capas_2000_2015 = albimanus_capas[(albimanus_capas.year >= 2000) & (albimanus_capas.year < 2015)]
albimanus_capas_2015_2024 = albimanus_capas[(albimanus_capas.year >= 2015) & (albimanus_capas.year <= 2024)]

"""---

#### Guarda .csv
"""

albimanus_capas.to_csv('albimanus_capas.csv', index=False)
albimanus_capas_1902_1970.to_csv('albimanus_capas(1902-1970).csv', index=False)
albimanus_capas_1970_1985.to_csv('albimanus_capas(1970-1985).csv', index=False)
albimanus_capas_1985_2000.to_csv('albimanus_capas(1985-2000).csv', index=False)
albimanus_capas_2000_2015.to_csv('albimanus_capas(2000-2015).csv', index=False)
albimanus_capas_2015_2024.to_csv('albimanus_capas(2015-2024).csv', index=False)

"""---

### Hendersoni

#### Concatenación de datos
"""

hendersoni_capas = pd.read_csv('/content/drive/MyDrive/Ecologia/Dataset/dataset_capas/Hendersoni.csv')

"""Vemos que coincide el orden de la información del Datframe que contiene los valores de las capas bioclimáticas de cada individuo con la información general antes seleccionada del individuo. Por lo que concatenamos estos dos Dataframes"""

#comprobación del orde
hendersoni1 = hendersoni_capas[['X', 'Y']]
hendersoni2 = hendersoni[['decimalLongitude', 'decimalLatitude']]

hendersoni1.reset_index(drop=True, inplace=True)
hendersoni2.reset_index(drop=True, inplace=True)

pd.concat([hendersoni1, hendersoni2], axis=1)

#borro la columna X, Y de hendersoni_capas
hendersoni_capas.drop('X', axis = 1, inplace =True)
hendersoni_capas.drop('Y', axis = 1, inplace = True)

hendersoni_capas.reset_index(drop=True, inplace=True)

hendersoni.reset_index(drop=True, inplace=True)

hendersoni_capas = pd.concat([hendersoni, hendersoni_capas], axis=1); hendersoni_capas

"""---

#### Segementación por año
>[1970-1985)
>
>[1985-2000)
>
>[2000-2012)
>
>[2015-2024]
"""

#borramos la columna fid
hendersoni_capas = hendersoni_capas.drop('fid', axis =1)

hendersoni_capas_1970_1985 = hendersoni_capas[(hendersoni_capas.year >= 1970) & (hendersoni_capas.year < 1985)]

"""---

#### Guarda .csv
"""

hendersoni_capas.to_csv('hendersoni_capas.csv', index=False)
hendersoni_capas_1970_1985.to_csv('hendersoni_capas(1970-1985).csv', index=False)

"""---

### Triseriatus

#### Concatenación de datos
"""

triseriatus_capas = pd.read_csv('/content/drive/MyDrive/Ecologia/Dataset/dataset_capas/Triseriatus.csv')

"""Vemos que coincide el orden de la información del Datframe que contiene los valores de las capas bioclimáticas de cada individuo con la información general antes seleccionada del individuo. Por lo que concatenamos estos dos Dataframes"""

#comprobación del orde
triseriatus1 = triseriatus_capas[['X', 'Y']]
triseriatus2 = triseriatus[['decimalLongitude', 'decimalLatitude']]

triseriatus1.reset_index(drop=True, inplace=True)
triseriatus2.reset_index(drop=True, inplace=True)

pd.concat([triseriatus1, triseriatus2], axis=1)

#borro la columna X, Y de triseriatus_capas
triseriatus_capas.drop('X', axis = 1, inplace =True)
triseriatus_capas.drop('Y', axis = 1, inplace = True)

triseriatus_capas.reset_index(drop=True, inplace=True)

triseriatus.reset_index(drop=True, inplace=True)

triseriatus_capas = pd.concat([triseriatus, triseriatus_capas], axis=1); triseriatus_capas

"""---

#### Datos nulos
"""

triseriatus_capas.isnull().sum()

#borramos los datos faltantes
triseriatus_capas = triseriatus_capas.dropna()
#borramos la columna fid
triseriatus_capas = triseriatus_capas.drop('fid', axis =1)

"""---

#### Segementación por año
>[1970-1985)
>
>[1985-2000)
>
>[2000-2012)
>
>[2015-2024]
"""

triseriatus_capas_1954_1970 = triseriatus_capas[(triseriatus_capas.year >= 1954) & (triseriatus_capas.year < 1970)]
triseriatus_capas_1985_2000 = triseriatus_capas[(triseriatus_capas.year >= 1985) & (triseriatus_capas.year < 2000)]
triseriatus_capas_2000_2015 = triseriatus_capas[(triseriatus_capas.year >= 200) & (triseriatus_capas.year < 20015)]

"""---

#### Guarda .csv
"""

triseriatus_capas.to_csv('triseriatus_capas.csv', index=False)
triseriatus_capas_1954_1970.to_csv('triseriatus_capas(1954-1970).csv', index=False)
triseriatus_capas_1985_2000.to_csv('triseriatus_capas(1985-2000).csv', index=False)
triseriatus_capas_2000_2015.to_csv('triseriatus_capas(2000-2015).csv', index=False)

"""---

### Pseudopunctipennis

#### Concatenación de datos
"""

pseudopunctipennis_capas = pd.read_csv('/content/drive/MyDrive/Ecologia/Dataset/dataset_capas/Pseudopunctipennis.csv')

"""Vemos que coincide el orden de la información del Datframe que contiene los valores de las capas bioclimáticas de cada individuo con la información general antes seleccionada del individuo. Por lo que concatenamos estos dos Dataframes"""

#comprobación del orde
pseudopunctipennis1 = pseudopunctipennis_capas[['X', 'Y']]
pseudopunctipennis2 = pseudopunctipennis[['decimalLongitude', 'decimalLatitude']]

pseudopunctipennis1.reset_index(drop=True, inplace=True)
pseudopunctipennis2.reset_index(drop=True, inplace=True)

pd.concat([pseudopunctipennis1, pseudopunctipennis2], axis=1)

#borro la columna X, Y de pseudopunctipennis_capas
pseudopunctipennis_capas.drop('X', axis = 1, inplace =True)
pseudopunctipennis_capas.drop('Y', axis = 1, inplace = True)

pseudopunctipennis_capas.reset_index(drop=True, inplace=True)

pseudopunctipennis.reset_index(drop=True, inplace=True)

pseudopunctipennis_capas = pd.concat([pseudopunctipennis, pseudopunctipennis_capas], axis=1); pseudopunctipennis_capas

"""---

#### Datos nulos
"""

pseudopunctipennis_capas.isnull().sum()

#borramos los datos faltantes
pseudopunctipennis_capas = pseudopunctipennis_capas.dropna()
#borramos la columna fid
pseudopunctipennis_capas = pseudopunctipennis_capas.drop('fid', axis =1)

"""---

#### Segementación por año
>[1970-1985)
>
>[1985-2000)
>
>[2000-2012)
>
>[2015-2024]
"""

pseudopunctipennis_capas_1925_1970 = pseudopunctipennis_capas[(pseudopunctipennis_capas.year >= 1925) & (pseudopunctipennis_capas.year < 1970)] #Se  guardan los datos que fueron  registrados antes de 1970
pseudopunctipennis_capas_1970_1985 = pseudopunctipennis_capas[(pseudopunctipennis_capas.year >= 1970) & (pseudopunctipennis_capas.year < 1985)]
pseudopunctipennis_capas_1985_2000 = pseudopunctipennis_capas[(pseudopunctipennis_capas.year >= 1985) & (pseudopunctipennis_capas.year < 2000)]
pseudopunctipennis_capas_2000_2015 = pseudopunctipennis_capas[(pseudopunctipennis_capas.year >= 2000) & (pseudopunctipennis_capas.year < 2015)]
pseudopunctipennis_capas_2015_2024 = pseudopunctipennis_capas[(pseudopunctipennis_capas.year >= 2015) & (pseudopunctipennis_capas.year <= 2024)]

"""---

#### Guarda .csv
"""

pseudopunctipennis_capas.to_csv('pseudopunctipennis_capas.csv', index=False)
pseudopunctipennis_capas_1925_1970.to_csv('pseudopunctipennis_capas(1925-1970).csv', index=False)
pseudopunctipennis_capas_1970_1985.to_csv('pseudopunctipennis_capas(1970-1985).csv', index=False)
pseudopunctipennis_capas_1985_2000.to_csv('pseudopunctipennis_capas(1985-2000).csv', index=False)
pseudopunctipennis_capas_2000_2015.to_csv('pseudopunctipennis_capas(2000-2015).csv', index=False)
pseudopunctipennis_capas_2015_2024.to_csv('pseudopunctipennis_capas(2015-2024).csv', index=False)

"""---"""
"""---

###Olmeca

#### Concatenación de datos
"""

olmeca_capas = pd.read_csv('/content/drive/MyDrive/Ecologia/Dataset/dataset_capas/Olmeca.csv')

"""Vemos que coincide el orden de la información del Datframe que contiene los valores de las capas bioclimáticas de cada individuo con la información general antes seleccionada del individuo. Por lo que concatenamos estos dos Dataframes"""

#comprobación del orde
olmeca1 = olmeca_capas[['X', 'Y']]
olmeca2 = olmeca[['decimalLongitude', 'decimalLatitude']]

olmeca1.reset_index(drop=True, inplace=True)
olmeca2.reset_index(drop=True, inplace=True)

pd.concat([olmeca1, olmeca2], axis=1)

#borro la columna X, Y de olmeca_capas
olmeca_capas.drop('X', axis = 1, inplace =True)
olmeca_capas.drop('Y', axis = 1, inplace = True)

olmeca_capas.reset_index(drop=True, inplace=True)

olmeca.reset_index(drop=True, inplace=True)

olmeca_capas = pd.concat([olmeca, olmeca_capas], axis=1); olmeca_capas

"""---

#### Segementación por año
>[1970-1985)
>
>[1985-2000)
>
>[2000-2012)
>
>[2015-2024]
"""

#borramos la columna fid
olmeca_capas = olmeca_capas.drop('fid', axis =1)

olmeca_capas_1985_2000 = olmeca_capas[(olmeca_capas.year >= 1985) & (olmeca_capas.year < 2000)]
olmeca_capas_2000_2015 = olmeca_capas[(olmeca_capas.year >= 2000) & (olmeca_capas.year < 2015)]

"""---

#### Guardar en .csv
"""

olmeca_capas.to_csv('olmeca_capas.csv', index=False)
olmeca_capas_1985_2000.to_csv('olmeca_capas(1985-2000).csv', index=False)
olmeca_capas_2000_2015.to_csv('olmeca_capas(2000-2015).csv', index=False)

"""---

### Cruciata

#### Concatenación de datos
"""

cruciata_capas = pd.read_csv('/content/drive/MyDrive/Ecologia/Dataset/dataset_capas/Cruciata.csv')

"""Vemos que coincide el orden de la información del Datframe que contiene los valores de las capas bioclimáticas de cada individuo con la información general antes seleccionada del individuo. Por lo que concatenamos estos dos Dataframes"""

#comprobación del orde
cruciata1 = cruciata_capas[['X', 'Y']]
cruciata2 = cruciata[['decimalLongitude', 'decimalLatitude']]

cruciata1.reset_index(drop=True, inplace=True)
cruciata2.reset_index(drop=True, inplace=True)

pd.concat([cruciata1, cruciata2], axis=1)

#borro la columna X, Y de cruciata_capas
cruciata_capas.drop('X', axis = 1, inplace =True)
cruciata_capas.drop('Y', axis = 1, inplace = True)

cruciata_capas.reset_index(drop=True, inplace=True)

cruciata.reset_index(drop=True, inplace=True)

cruciata_capas = pd.concat([cruciata, cruciata_capas], axis=1); cruciata_capas

"""---

#### Segementación por año
>[1970-1985)
>
>[1985-2000)
>
>[2000-2012)
>
>[2015-2024]
"""

#borramos la columna fid
cruciata_capas = cruciata_capas.drop('fid', axis =1)

cruciata_capas_1985_2000 = cruciata_capas[(cruciata_capas.year >= 1985) & (cruciata_capas.year < 2000)]
cruciata_capas_2000_2015 = cruciata_capas[(cruciata_capas.year >= 2000) & (cruciata_capas.year < 2015)]

"""---

#### Guardar en .csv
"""

cruciata_capas.to_csv('cruciata_capas.csv', index=False)
cruciata_capas_1985_2000.to_csv('cruciata_capas(1985-2000).csv', index=False)
cruciata_capas_2000_2015.to_csv('cruciata_capas(2000-2015).csv', index=False)

"""---

### Zoosophus

#### Concatenación de datos
"""

zoosophus_capas = pd.read_csv('/content/drive/MyDrive/Ecologia/Dataset/dataset_capas/Zoosophus.csv')

"""Vemos que coincide el orden de la información del Datframe que contiene los valores de las capas bioclimáticas de cada individuo con la información general antes seleccionada del individuo. Por lo que concatenamos estos dos Dataframes"""

#comprobación del orde
zoosophus1 = zoosophus_capas[['X', 'Y']]
zoosophus2 = zoosophus[['decimalLongitude', 'decimalLatitude']]

zoosophus1.reset_index(drop=True, inplace=True)
zoosophus2.reset_index(drop=True, inplace=True)

pd.concat([zoosophus1, zoosophus2], axis=1)

#borro la columna X, Y de zoosophus_capas
zoosophus_capas.drop('X', axis = 1, inplace =True)
zoosophus_capas.drop('Y', axis = 1, inplace = True)

zoosophus_capas.reset_index(drop=True, inplace=True)

zoosophus.reset_index(drop=True, inplace=True)

zoosophus_capas = pd.concat([zoosophus, zoosophus_capas], axis=1); zoosophus_capas

"""---

#### Segementación por año
>[1970-1985)
>
>[1985-2000)
>
>[2000-2012)
>
>[2015-2024]
"""

#borramos la columna fid
zoosophus_capas = zoosophus_capas.drop('fid', axis =1)

zoosophus_capas_1961_1970 = zoosophus_capas[(zoosophus_capas.year >= 1961) & (zoosophus_capas.year < 1970)]
zoosophus_capas_2000_2015 = zoosophus_capas[(zoosophus_capas.year >= 2000) & (zoosophus_capas.year < 2015)]

"""---

#### Guardar en .csv
"""

zoosophus_capas.to_csv('zoosophus_capas.csv', index=False)
zoosophus_capas_1961_1970.to_csv('zoosophus_capas(1961-1970).csv', index=False)
zoosophus_capas_2000_2015.to_csv('zoosophus_capas(2000-2015).csv', index=False)

"""---"""


